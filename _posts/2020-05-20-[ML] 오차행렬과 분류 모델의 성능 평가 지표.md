---
layout: post
title:  "[ML] 오차행렬과 분류 모델의 성능 평가 지표"
date:   2020-05-20 12:53:25
author: zeroaan
categories: ml/dl
use_math: true
# tags: python code
# cover:  "/assets/header_image.jpg"
---

## 분류성능평가지표 - Precision(정밀도), Recall(재현율), Accuracy(정확도), F1-score

![confusion]({{site.baseurl}}/img/confusion.png)

- True Positive(TP) : 실제 True인 정답을 True라고 예측 (정답)
- True Negative(TN) : 실제 False인 정답을 False라고 예측 (정답)
- False Positive(FP) : 실제 False인 정답을 True라고 예측 (오답)
- False Negative(FN) : 실제 True인 정답을 False라고 예측 (오답)

<br>

### 1) Accuracy(정확도)
정확도는 **전체 예측 건수에서 정답을 맞힌 건수의 비율**이다.(정답이 True, False 상관 없음)

$$ Accuracy =  \frac {TP+TN} {TP+FN+FP+TN} $$

정확도는 가장 직관적으로 모델의 성능을 나타낼 수 있는 평가 지표이다.<br>
쉽게 말하면 정확도는 예측이 정답과 얼마나 정확한지를 나타낸다.


하지만 accuracy는 데이터에 따라 잘못된 통계를 가져올 수도 있다. 예를 들어 감염률이 0.1%인 질병을 예측하는 모델이 있다고 가정해본다.
감염률이 1%도 안되기 때문에 무조건 Negative로 예측하면 매우 높은 accuracy가 나올 것이다.<br>

하지만 높은 accuracy가 나오지만 True Positive는 거의 없이 True Negative만 높아지게 되므로 이 모델은 전혀 쓸모가 없다.
이렇게 실제 데이터에 Negative 비율이 너무 높아서 희박한 가능성으로 발생할 상황에 대해 제대로된 분류를 해주는지 평가해줄 다른 지표들을 찾아야한다.

<br>

### 2) Recall(재현율)
재현율은 **실제 정답이 True인 것 중에서 분류기가 True라고 예측한 것의 비율**이다. 

$$ Recall =  \frac {TP} {TP+FN} $$

sensitivity(민감도), hit rate(적중률), True Positive Rate(진짜 양성 비율) 이라고도 부른다.<br>
쉽게 말하면 재현율은 찾아야 할 것중에 실제로 찾은 비율을 말한다.


위에서 예로 들었던 질병을 예측하는 모델에서 언제나 Negative로 예측하는 분류기가 있다면 accuracy는 거의 100%에 가까운 값이 나오겠지만, 
recall은 실제 정답이 true인 것 중에서 모델이 true라고 예측한 것의 비율이므로 True Positive를 찾을 수 없으니 recall은 0이 된다.

하지만 반대로 분류기가 항상 true로 예측한다면 accuracy는 낮아지겠지만, recall은 높은 값을 가진다.

<br>

### 3) Precision(정밀도)
정밀도란 **분류기에 예측이 True인 것 중에서 실제 정답이 True인 경우의 비율**이다.

$$ Precision =  \frac {TP} {TP+FP} $$

Positive 정답률, Positive Predictive Value(양성 예측도) 라고도 부른다.<br>
쉽게 말해서 정밀도는 예측한 것중에 정답의 비율을 말한다.


위에서 예로 들었던 질병 예측 모델에서 항상 true로 예측하는 분류기에서 모델이 항상 질병이라고 예측하기 때문에 recall은 1에 가까운 값을 가지지만, 
실제로 질병이 있을 확률이 거의 없기 때문에 precision은 0에 가까울 것이다. 

<br>

### Recall 과 Precision의 차이
Recall이나 Precision은 모두 실제 True인 정답을 모델이 True라고 예측한 경우에 관심이 있으나, 바라보고자 하는 관점이 다르다.<br>
Recall은 실제 정답의 입장에서, Precision은 예측 모델의 입장에서 정답을 정답이라고 맞춘 경우를 바라보고 있다.

먼저 코로나를 판단 하는 모델로 예를 들어보겠다.

- TP : 코로나 환자에게 코로나라고 예측한 경우
- FN : 코로나 환자에게 코로나가 아니라고 예측한 경우
- FP : 코로나 환자가 아닌 사람에게 코로나라고 예측한 경우
- TN : 코로나 환자가 아닌 사람에게 코로나가 아니라고 예측한 경우

여기서 TP와 TN은 정확히 예측한 경우이고 FP와 FN은 잘못 예측한 경우이다.<br>
하지만 FN에서 코로나 환자에게 코로나가 아니라고 예측했을 때 최악의 문제가 생기게 된다.<br>

이런 경우에 Recall(재현율)을 사용한다.

<br>

다음으로 스팸 메일 구분하는 모델로 예를 들어보겠다.

- TP : 스팸 메일을 스팸으로 예측한 경우
- FN : 스팸 메일을 정상으로 예측한 경우
- FP : 정상 메일을 스팸으로 예측한 경우
- TN : 정상 메일을 정상으로 예측한 경우

여기서도 TP와 TN은 정확히 예측한 경우이고 FP와 FN은 잘못 예측한 경우이다.<br>
하지만 이 모델에서는 FP에서 정상 메일을 스팸 메일로 예측했을 때 정상적인 업무가 불가능하게 된다.<br>

이런 경우에 Precision(정밀도)를 사용한다.

<br>

쉽게 말해서 재현율은 잘못 걸러내는 비율이 높더라도 놓치는 것이 없도록 하는 것이고 
정밀도는 미처 잡아내지 못하는 것이 있더라도 더 정확한 예측이 필요할 때 사용한다.

<br>

가장 좋은 것은 Recall과 Precision 모두 높은 값을 얻는 것이다.<br>
하지만 Recall과 Precision은 상충하는 개념이기 때문에 하나가 높아지면 다른 하나는 낮아진다.

그래서 이 둘을 조합해서 만든 평가 지표가 F1-score 이다.

<br>

### 4) F1 score
F1 score는 **Recall과 Precision의 조화평균**이다. 

$$ F1-score = 2 * \frac  {1} {\frac {1}{Presicion} + \frac {1}{Recall}} = 2 * \frac {Presicion * Recall}{Presicion+Recall} $$

F1 score는 데이터가 불균형할 때 모델의 성능을 정확하게 평가할 수 있다.

산술평균을 사용하지 않고 조화평균을 사용하는 이유는 예를 들어 recall = 1, precision = 0.01 이라고 하고 평균을 구해보자.

산술평균을 구하면 $ \frac {(1 + 0.01)} {2} = 0.505 $ 가 나오게 되고 이 모델이 절반은 맞추는 것처럼 보인다.

하지만 조화평균을 구하면 $ 2 * \frac {(1 * 0.01)} {(1 + 0.01)} = 0.019 $ 이므로 조화평균을 이용하여 평가를 한 것이 성능이 더 좋음을 확인할 수 있다.

<br>

## 그 외 다른 지표들
이 외에도 모델의 성능을 측정하는 다양한 지표들이 존재한다.

### 1)  Fall-out
Fall-out은 FPR(False Positive Rate)으로도 불리며, **실제 False인 것 중에서 모델이 True라고 예측한 비율**이다. 
즉, 모델이 실제 false data인데 True라고 잘못 예측한 것으로 다음과 같이 표현할 수 있다.

$$ fall-out(FPR) =  \frac {FP} {TN+FP} $$

<br>

### 2) ROC(Receiver Operating Characteristic) curve
여러 값들을 기준으로 **Recall - Fallout의 변화를 시각화**한 것이다.<br>
즉, TPR(진짜 양성 비율)에 대한 FPR(거짓 양성 비율) 을 나타낸다.

Fallout은 실제 False인 data 중에서 모델이 True로 분류한 것이고,<br>
Recall은 실제 True인 data 중에서 모델이 True로 분류한 비율을 나타낸 지표이다.

이 두 지표를 각각 x, y의 축으로 놓고 그래프가 그려진다.

![roccurve]({{site.baseurl}}/img/roccurve.png)

curve가 왼쪽 위 모서리에 가까울수록 모델의 성능이 좋다고 평가한다.<br>
즉, Recall(TPR)이 크고 Fall-out(FPR)이 작은 모형이 좋은 모형이다.

<br>

### 3) AUC(Area Under Curve)
AUC 는 **ROC 곡선 아래 영역**을 의미한다.

ROC curve는 그래프이기 때문에 명확한 수치로써 비교하기가 어렵다. 따라서 그래프 아래의 면적값을 이용한다.

![auc]({{site.baseurl}}/img/auc.png)

AUC 값의 범위는 0 ~ 1 이다. 예측이 잘못된 모델의 AUC는 0에 가깝고 예측이 정확한 모델의 AUC는 1에 가까운 값이 나온다.

즉, Fall-out(FPR) 보다 Recall(TPR) 의 값이 클수록 AUC 가 높아지고 예측이 정확한 모델이다.

<br>

#### References 
- https://sumniya.tistory.com/26
- https://subinium.github.io/basic-of-Evaluation/
- https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5