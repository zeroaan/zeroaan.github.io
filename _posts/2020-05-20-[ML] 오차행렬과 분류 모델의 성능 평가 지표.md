---
layout: post
title:  "[ML] 오차행렬과 분류 모델의 성능 평가 지표"
date:   2020-05-20 12:53:25
author: zeroaan
categories: ml/dl
# tags: python code
# cover:  "/assets/header_image.jpg"
---

## 분류성능평가지표 - Precision(정밀도), Recall(재현율), Accuracy(정확도), F1-score

![confusion]({{site.baseurl}}/img/confusion.png)

- True Positive(TP) : 실제 True인 정답을 True라고 예측 (정답)
- True Negative(TN) : 실제 False인 정답을 False라고 예측 (정답)
- False Positive(FP) : 실제 False인 정답을 True라고 예측 (오답)
- False Negative(FN) : 실제 True인 정답을 False라고 예측 (오답)

<br>

### 1) Accuracy(정확도)
정확도는 **전체 예측 건수에서 정답을 맞힌 건수의 비율**이다.(정답이 True, False 상관 없음)

$$ Accuracy =  \frac {TP+TN} {TP+FN+FP+TN} $$

정확도는 가장 직관적으로 모델의 성능을 나타낼 수 있는 평가 지표이다.<br>
쉽게 말하면 정확도는 예측이 정답과 얼마나 정확한지를 나타낸다.


하지만 accuracy는 데이터에 따라 잘못된 통계를 가져올 수도 있다. 예를 들어 내일 태풍이 발생할지 예측하는 모델이 있다고 가정해본다.<br>
태풍은 거의 발생하지 않으므로 무조건 Negative로 예측하면 매우 높은 accuracy가 나올 것이다.<br>

하지만 높은 accuracy가 나오지만 True Positive는 하나도 없이 True Negative만 높아지게 되므로 이 모델은 전혀 쓸모가 없다.<br>
이렇게 실제 데이터에 Negative 비율이 너무 높아서 희박한 가능성으로 발생할 상황에 대해 제대로된 분류를 해주는지 평가해줄 다른 지표들을 찾아야한다.

<br>

### 2) Recall(재현율)
재현율은 **실제 정답이 True인 것 중에서 분류기가 True라고 예측한 것의 비율**이다. 

$$ Recall =  \frac {TP} {TP+FN} $$

sensitivity(민감도), hit rate(적중률), True Positive Rate(진짜 양성 비율) 이라고도 부른다.<br>
쉽게 말하면 재현율은 찾아야 할 것중에 실제로 찾은 비율을 말한다.


위에서 예로 들었던 태풍이 발생할지 예측하는 모델에서 언제나 Negative로 예측하는 분류기가 있다면 accuracy는 거의 99%를 넘기겠지만,<br>
recall은 실제 정답이 true인 것 중에서 모델이 true라고 예측한 것의 비율이므로 True Positive를 찾을 수 없으니 recall은 0이 된다.

하지만 반대로 분류기가 항상 true로 예측한다면 accuracy는 낮아지겠지만, recall은 1이 된다.

<br>

### 3) Precision(정밀도)
정밀도란 **분류기에 예측이 True인 것 중에서 실제 정답이 True인 경우의 비율**이다.

$$ Precision =  \frac {TP} {TP+FP} $$

Positive 정답률, Positive Predictive Value(양성 예측도) 라고도 부른다.<br>
쉽게 말해서 정밀도는 예측한 것중에 정답의 비율을 말한다.


위에서 예로 들었던 태풍 예측 모델에서 항상 true로 예측하는 분류기에서 모델이 항상 태풍이 올거라 예측하기 때문에 recall은 1이지만,<br>
실제로 태풍이 발생하는 날은 거의 없기 때문에 precision은 0에 가까울 것이다. 

<br>

Recall과 Precision은 상충하는 개념이기 때문에 하나가 높아지면 다른 하나는 낮아진다.<br>
Recall이나 Precision은 모두 실제 True인 정답을 모델이 True라고 예측한 경우에 관심이 있으나, 바라보고자 하는 관점이 다르다.<br>
Recall은 실제 정답의 입장에서, Precision은 예측 모델의 입장에서 정답을 정답이라고 맞춘 경우를 바라보고 있다.

<br>

### 4) F1 score
F1 score는 **Recall과 Precision의 조화평균**이다. 

$$ F1-score = 2 * \frac  {1} {\frac {1}{Presicion} + \frac {1}{Recall}} = 2 * \frac {Presicion * Recall}{Presicion+Recall} $$

F1 score는 데이터가 불균형할 때 모델의 성능을 정확하게 평가할 수 있다.

산술평균을 사용하지 않고 조화평균을 사용하는 이유는 예를 들어 recall = 1, precision = 0.01 이라고 하고 평균을 구해보자.

산술평균을 구하면 $ \frac {(1 + 0.01)} {2} = 0.505 $ 가 나오게 되고 이 모델이 절반은 맞추는 것처럼 보인다.

하지만 조화평균을 구하면 $ 2 * \frac {(1 * 0.01)} {(1 + 0.01)} = 0.019 $ 이므로 조화평균을 이용하여 평가를 한 것이 성능이 더 좋음을 확인할 수 있다.

<br>

## 그 외 다른 지표들
이 외에도 모델의 성능을 측정하는 다양한 지표들이 존재한다.

### 1)  Fall-out
Fall-out은 FPR(False Positive Rate)으로도 불리며, **실제 False인 것 중에서 모델이 True라고 예측한 비율**이다. <br>
즉, 모델이 실제 false data인데 True라고 잘못 예측한 것으로 다음과 같이 표현할 수 있다.

$$ fall-out(FPR) =  \frac {FP} {TN+FP} $$

<br>

### 2) ROC(Receiver Operating Characteristic) curve
여러 값들을 기준으로 **Recall - Fallout의 변화를 시각화**한 것이다.<br>
즉, TPR(진짜 양성 비율)에 대한 FPR(거짓 양성 비율) 을 나타낸다.

Fallout은 실제 False인 data 중에서 모델이 True로 분류한 것이고,<br>
Recall은 실제 True인 data 중에서 모델이 True로 분류한 비율을 나타낸 지표이다.

이 두 지표를 각각 x, y의 축으로 놓고 그래프가 그려진다.

![image](./img/roccurve.png)

curve가 왼쪽 위 모서리에 가까울수록 모델의 성능이 좋다고 평가한다.<br>
즉, Recall(TPR)이 크고 Fall-out(FPR)이 작은 모형이 좋은 모형이다.

<br>

### 3) AUC(Area Under Curve)
AUC 는 **ROC 곡선 아래 영역**을 의미한다.

ROC curve는 그래프이기 때문에 명확한 수치로써 비교하기가 어렵다. 따라서 그래프 아래의 면적값을 이용한다.

![image](./img/auc.png)

AUC 값의 범위는 0 ~ 1 이다. 예측이 잘못된 모델의 AUC는 0에 가깝고 예측이 정확한 모델의 AUC는 1에 가까운 값이 나온다.

즉, Fall-out(FPR) 보다 Recall(TPR) 의 값이 클수록 AUC 가 높아지고 예측이 정확한 모델이다.

<br>

> References 
- https://sumniya.tistory.com/26
- https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5